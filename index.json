[{"authors":null,"categories":null,"content":"I‚Äôm a postdoctoral researcher at the Data and Web Science Group at the University of Mannheim.\nI am interested in large corpora for training language models, specially for under resourced languages and historical languages. I am interested in tasks such as Name Entity Recognition (NER), Dependency Parsing and Part-of-Speech tagging, Machine Translation and Document structuration.\nI love coffee, cookies and maths. ‚òïüç™\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"d0afc1aafe936456114b5d845db0a44a","permalink":"https://oscar-project.org/authors/pedro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pedro/","section":"authors","summary":"I‚Äôm a postdoctoral researcher at the Data and Web Science Group at the University of Mannheim.\nI am interested in large corpora for training language models, specially for under resourced languages and historical languages.","tags":null,"title":"Pedro Ortiz Suarez","type":"authors"},{"authors":null,"categories":null,"content":"I am a Ph.D. student at the Data and Web Science Group at the University of Mannheim.\nMy current interest is automatic summarization systems for research papers. Additionally, to academic side of the field, I also like to develop webpages and software.\nI love coffee, reading papers and books.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"247912bed8bf7a4a8f6cd01564c97a4b","permalink":"https://oscar-project.org/authors/contrib-sotaro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/contrib-sotaro/","section":"authors","summary":"I am a Ph.D. student at the Data and Web Science Group at the University of Mannheim.\nMy current interest is automatic summarization systems for research papers. Additionally, to academic side of the field, I also like to develop webpages and software.","tags":null,"title":"Sotaro Takeshita","type":"authors"},{"authors":null,"categories":null,"content":"I‚Äôm a research engineer working on better software to generate, manage and filter large corpora. I use Rust and Python to improve performance and safety of code and generate statistical data about generated corpora.\nI‚Äôm interested in bringing more software engineering practices in research software in order to improve performance and reliability. I am currently employed at the ALMAnaCH research team at Inria.\nI like movies, making music and eating falafel üßÜ.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"d0f9b85571ca19bff50c8d5ae6303df4","permalink":"https://oscar-project.org/authors/julien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/julien/","section":"authors","summary":"I‚Äôm a research engineer working on better software to generate, manage and filter large corpora. I use Rust and Python to improve performance and safety of code and generate statistical data about generated corpora.","tags":null,"title":"Julien Abadji","type":"authors"},{"authors":null,"categories":null,"content":"I am a Data Scientist at H\u0026amp;M Group in Stockholm, working in the AI Research team.\nMy current research interests are generative models, especially normalizing flows, and language models.\nI strive to make self-developed AI models usable by everyone through simple APIs and standardized processes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"3340105e5e3dc183a801b6fa2ade3e7e","permalink":"https://oscar-project.org/authors/contrib-patrick/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/contrib-patrick/","section":"authors","summary":"I am a Data Scientist at H\u0026M Group in Stockholm, working in the AI Research team.\nMy current research interests are generative models, especially normalizing flows, and language models.\nI strive to make self-developed AI models usable by everyone through simple APIs and standardized processes.","tags":null,"title":"Patrick Teufert","type":"authors"},{"authors":null,"categories":null,"content":"I am a Research Engineer at ALMAnaCH Inria team, I am interested in natural language processing for low resources languages and dialects. I have a MSc in Machine Learning and B.Sc in Statistics and Computer Science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"1ba0c8fbaa2d32ebec8bc48fb455f6db","permalink":"https://oscar-project.org/authors/rua/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rua/","section":"authors","summary":"I am a Research Engineer at ALMAnaCH Inria team, I am interested in natural language processing for low resources languages and dialects. I have a MSc in Machine Learning and B.","tags":null,"title":"Rua Ismail","type":"authors"},{"authors":null,"categories":null,"content":"Laurent Romary is Directeur de Recherche at Inria, France and director general of DARIAH. He received a PhD degree in computational linguistics in 1989 and his Habilitation in 1999. He carries out research on the modelling of semi-structured documents, with a specific emphasis on texts and linguistic resources. He has been active in standardisation activities with ISO, as chair of committee ISO/TC 37/SC 4 (2002-2014), chair of ISO/TC 37 (2016-) and the Text Encoding Initiative, as member (2001-2011) and chair (2008-2011) of its technical council. He has been involved in the definition of the scientific information policy of CNRS (2005-2006), the Max-Planck Digital Library (2006-2008) and Inria (2006-).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"5aa5bdf7fb31aded56ed5ea9447af54c","permalink":"https://oscar-project.org/authors/laurent/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/laurent/","section":"authors","summary":"Laurent Romary is Directeur de Recherche at Inria, France and director general of DARIAH. He received a PhD degree in computational linguistics in 1989 and his Habilitation in 1999. He carries out research on the modelling of semi-structured documents, with a specific emphasis on texts and linguistic resources.","tags":null,"title":"Laurent Romary","type":"authors"},{"authors":null,"categories":null,"content":"Inria Senior Researcher in Natural Language Processing and Computational Linguistics. Head of the ALMAnaCH research team. PRAIRIE chair.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670526880,"objectID":"2d1d85691b221638c9407df7e99b0a55","permalink":"https://oscar-project.org/authors/benoit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/benoit/","section":"authors","summary":"Inria Senior Researcher in Natural Language Processing and Computational Linguistics. Head of the ALMAnaCH research team. PRAIRIE chair.","tags":null,"title":"Beno√Æt Sagot","type":"authors"},{"authors":null,"categories":null,"content":"OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.\nGrab the latest OSCAR release here! Join our Discord community here! Data is distributed by language in both original and deduplicated form. There are currently 166 different languages available. If you use OSCAR please consider giving us some feedback by writing to our mail address down below. Also consider citing our papers.\nIf you want to contribute to OSCAR, for example by tokenizing one of the corpora for a particular language, or by helping us translate our webpage, please open a pull request here.\nThe corpus was put together by Pedro Ortiz Suarez, Julien Abadji, Beno√Æt Sagot, and Laurent Romary. OSCAR is mainly funded by Inria (project-team ALMAnaCH) and by the PRAIRIE institute.\nIf you are interested in OSCAR and would like to access the corpus, send us a mail using the mail address down below, with ‚ÄúOSCAR Access Request‚Äù as mail title. Please include your name, last name, affiliation, contact details, which languages do you need and a brief description of how you intend to use OSCAR.\nEven though OSCAR is not Postcardware, we do appreciate when our users send us a postcard. If you want to send us one, you can find the address in the contact section down below.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1673969237,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://oscar-project.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.\nGrab the latest OSCAR release here!","tags":null,"title":"OSCAR","type":"authors"},{"authors":["Julien Abadji","Pedro Ortiz Suarez","Laurent Romary","Beno√Æt Sagot"],"categories":null,"content":"","date":1642457579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647269568,"objectID":"40179a8d702c46ef8f54483a0a9bafa8","permalink":"https://oscar-project.org/publication/2022/arxiv/towards/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2022/arxiv/towards/","section":"publication","summary":"we take the existing multilingual web corpus OSCAR and its pipeline Ungoliant that extracts and classifies data from Common Crawl at the line level, and propose a set of improvements and automatic annotations in order to produce a new document-oriented version of OSCAR.","tags":null,"title":"Towards a Cleaner Document-Oriented Multilingual Crawled Corpus","type":"publication"},{"authors":["Julien Abadji","Pedro Ortiz Suarez"],"categories":["corpus","update"],"content":" If you want to get the new corpus please send us a mail using the mail in our homepage, with ‚ÄúOSCAR Access Request‚Äù as mail title. We will need the following information to properly create your account. Missing information could delay your access by days:\nFirst and last name: Affiliation: Contact details: Corpus version (or all if you need multiple ones): Languages: ‚úâÔ∏èüìö\nPlease do not create a new Huma-Num account by yourself, this will delay your access to the corpus by weeks! We will create an account for you. üìÜ OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using Ungoliant.\nThis release of OSCAR is the third one, and is versioned using CalVer.\nVersion info These are the versions of tooling, schemes and data\nCommonCrawl version: November/December 2021 (2021.49) OSCAR Schema version: v2 : Document oriented, with text and metadata merged in JSONLines. Ungoliant version: v1.1.0 : Document oriented pipeline, multilingual subcorpus, annotations Changes The corpus is not backward compatible, as the metadata and textual content have been merged into JSON objects. Document-oriented: Identifications are done per document, based on the proportion of content in a single language. Annotations: Annotations are labels you can filter on, potentially enabling better quality control (sacrificing corpus size). Per-line identification: Metadata hold line-level identification/confidence. Multilingual subcorpus: A new, 12GB multilingual subcorpus containing documents with sentences in different languages in significant proportion. Removed languages: Bavarian, Chavacano, Northern Frisian, Manx, Haitian Creole, Interlingue, Northern Luri, Mirandese, Eryza, Neapolitan, Pampanga, Romansh, Rusyn, Scots, Tuvinian, Venetian, West Flemish. No deduplicated corpus: Since the subcorpora are document oriented, doing a subcorpus-wide deduplication would break documents‚Äô integrity. We may consult the community and provide tools for deduplication later on. Data layout The corpus is now distributed in JSONLines format, which means that each line represents a single document, encoded in a JSON object.\nA text extraction utility is being added in the oscar-tools binary, enabling the creation of plain-text datasets from JSONL subcorpora.\n{ \u0026#34;content\u0026#34;:\u0026#34;newline\\nseparaaaaaaaaaaated\\ncontent\u0026#34;, //content itself // Headers from the crawler // Note that nothing is changed, so the content length may be incorrect. \u0026#34;warc_headers\u0026#34;:{ \u0026#34;warc-refers-to\u0026#34;:\u0026#34;\u0026lt;urn:uuid:83f2e1d4-5ed3-41db-86ff-f7826c4c20f9\u0026gt;\u0026#34;, \u0026#34;warc-date\u0026#34;:\u0026#34;2021-09-16T11:07:14Z\u0026#34;, \u0026#34;warc-block-digest\u0026#34;:\u0026#34;sha1:X3OWP47FG2O5LBNMFSNB44FJF2SSRC26\u0026#34;, \u0026#34;warc-type\u0026#34;:\u0026#34;conversion\u0026#34;, \u0026#34;warc-identified-content-language\u0026#34;:\u0026#34;eng\u0026#34;, \u0026#34;content-length\u0026#34;:\u0026#34;1694\u0026#34;, \u0026#34;warc-target-uri\u0026#34;:\u0026#34;https://foo.bar\u0026#34;, \u0026#34;warc-record-id\u0026#34;:\u0026#34;\u0026lt;urn:uuid:3304bc27-17d0-4ffd-a692-340381478a5f\u0026gt;\u0026#34;, \u0026#34;content-type\u0026#34;:\u0026#34;text/plain\u0026#34; }, \u0026#34;metadata\u0026#34;:{ // Document-wide identification. // The \u0026#34;prob\u0026#34; is the weighted average of the identified lines. \u0026#34;identification\u0026#34;:{ \u0026#34;label\u0026#34;:\u0026#34;en\u0026#34;, \u0026#34;prob\u0026#34;:0.6268374 }, // Annotations. Can be null if no annotations have been added. \u0026#34;annotation\u0026#34;:[ \u0026#34;short_sentences\u0026#34;, \u0026#34;footer\u0026#34; ], // Line-by-line identifications // Can have null values for lines that did not get an identification. \u0026#34;sentence_identifications\u0026#34;:[ { \u0026#34;label\u0026#34;:\u0026#34;en\u0026#34;, \u0026#34;prob\u0026#34;:0.93925816 }, null, { \u0026#34;label\u0026#34;:\u0026#34;en\u0026#34;, \u0026#34;prob\u0026#34;:0.9606543 } ] } } Annotations As a first step towards a better filtering of the corpus, we introduce annotations as tools to filter the corpus depending on users‚Äô criteria.\nThese annotations are imperfect and we will work on improving their usefulness.\ntiny: The document has a low (\u0026lt;5) number of lines. short_sentences: The document has a high number (\u0026gt;50%) of short lines (\u0026lt;400 bytes) header: The document has a high number of short lines at its head, suggesting the presence of low quality content. footer: The document has a high number of short lines at its tail, suggesting the presence of low quality content. noisy: The document has a high percentage of punctuation (\u0026gt;50%) adult: The document contains adult content. This annotation uses a blocklist and labels a tiny part of the corpus: It does not catch most of the adult content. More information about the thresholds and annotators are present in our paper.\nAbout the absence of some low-resource languages A number of low resource languages have disappeared or shrunk dramatically. This is due to the new document-level language identification, which may shadow low-resource languages that exhibit a linguistic proximity with higher-resourced ones.\nAs an example, Swiss German went from 5MB (21.09) to 360KB (22.01), but we found that the German (500GB) subcorpus contained around 30MB of Swiss German (without filtering out the sentences classified as \u0026lt;0.8 confidence).\nIt should be possible to rebuild or increase the size of low resource corpora by looking into other corpora and filtering on identifications.\nWe are working on ‚Ä¶","date":1641042131,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653401767,"objectID":"9870eb920afa90636d61014f19304278","permalink":"https://oscar-project.org/post/oscar-v22-01/","publishdate":"2022-01-01T15:02:11+02:00","relpermalink":"/post/oscar-v22-01/","section":"post","summary":"The January, 2022 version of the OSCAR Corpus.","tags":[],"title":"OSCAR 22.01","type":"post"},{"authors":null,"categories":null,"content":" Table of Contents How can I get the original files using the HuggingFace datasets platform? How can I decompress OSCAR Corpus files? Having a question or an issue with OSCAR? How can I get the original files using the HuggingFace datasets platform? HuggingFace‚Äôs datasets library internally uses Apache arrow files, different from the txt.gz/jsonl.gz pair that we use. These .arrow files can usually be found in ~/.cache/huggingface/datasets/ subfolders.\nHowever, it is possible to get the original (txt.gz/jsonl.gz) files using Git LFS.\nThe following steps assume you have git and git-lfs installed, and are on a UNIX system. The procedure should roughly be the same on Windows, but hasn‚Äôt been attempted.\n$\u0026gt; GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/oscar-corpus/OSCAR-2109 # clone the repository, ignoring LFS files $\u0026gt; cd OSCAR-2109 # go inside the directory $\u0026gt; git lfs pull --include packaged/eu/eu.txt.gz # pull the required file(s) (here the Basque corpus). Check with the manpage for pull options You‚Äôre all set! Decompress the files and you are ready to use the corpus.\nHow can I decompress OSCAR Corpus files? OSCAR is distributed in split files that are compressed in order to save spave. However, some decompression programs have trouble dealing with gz files. Here are some programs that work well for the three mainstream operating systems:\nOSX/Linux-based: Use gzip -d FILE.gz. Windows: Use 7zip Having a question or an issue with OSCAR? Send us your question by mail using the mail address in our homepage\n","date":1630658036,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639068424,"objectID":"4a1318da06786ac52626178ca960109e","permalink":"https://oscar-project.org/faq/","publishdate":"2021-09-03T10:33:56+02:00","relpermalink":"/faq/","section":"","summary":"Table of Contents How can I get the original files using the HuggingFace datasets platform? How can I decompress OSCAR Corpus files? Having a question or an issue with OSCAR?","tags":null,"title":"Frequently Asked Questions","type":"page"},{"authors":["Julien Abadji","Pedro Ortiz Suarez"],"categories":["news"],"content":"September 2021 marks an important milestone regarding OSCAR, and we have important news to share.\nNew pipeline tool The previous OSCAR corpus was generated by goclassy.\nThe latest (at the time of writing) and future ones will be generated by Ungoliant, a brand new corpus generation tool written in Rust.\nThe tool provides a more accessible Command Line Interface, with the following features:\nDowloading of CommonCrawl shards from a wet.paths file, Generation of a corpus from CommonCrawl shards, Deduplication, Splitting in files of fixed maxiumum size, Packaging (GZipping + checksum file creation) It is also possible (but not yet used/tested in terms of ergonomics) to use Ungoliant as a library.\nNew schema OSCAR is changing and will change again on the course of the following months/years, and as such it is important to provide a way to announce and specify schema changes.\nThe new OSCAR Corpus release follows the OSCAR Schema v1.1, enabling users to optionally get metadata extracted from CommonCrawl, while retaining backward compatibility, making the update from OSCAR 2018 to OSCAR 21.09 as effortless as possible.\nIn a gist, OSCAR Schema v1.1 adds \u0026lt;lang\u0026gt;_meta.jsonl JSONLines files that holds metadata. Each line has an offset and an nb_sentences field that can be used to get related lines in text files.\nNew corpus OSCAR 21.09 is the latest release of the OSCAR Corpus. The first to be generated by Ungoliant, and also the first containing metadata.\nNote that the data used to generate the corpus is from February 2021, but the next releases of OSCAR Corpus will try to narrow the gap between source data and corpus release.\nAnother important information is that there is no shuffled version anymore.\nIt is expected to be available during September 2021 via manual application through the Contact form of the website, and later on other platforms.\n","date":1630590355,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644600590,"objectID":"b14b90cb26f7528504366223e6832672","permalink":"https://oscar-project.org/post/news-21-09/","publishdate":"2021-09-02T15:45:55+02:00","relpermalink":"/post/news-21-09/","section":"post","summary":"New tools, metadata, and a fresh corpus.","tags":[],"title":"OSCAR News: September 2021","type":"post"},{"authors":["Julien Abadji","Pedro Ortiz Suarez"],"categories":["corpus","update"],"content":" If you want to get the new corpus please send us a mail using the mail in our homepage, with ‚ÄúOSCAR Access Request‚Äù as mail title. Please include your name, last name, affiliation, contact details, which languages do you need and a brief description of how you intend to use OSCAR. ‚úâÔ∏èüìö Please do not create a new Huma-Num account by yourself, this will delay your access to the corpus by weeks! We will create an account for you. üìÜ OSCAR 21.09 is now freely available on the Hugging Face‚Äôs datasets hub! üöÄ OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using Ungoliant.\nThis release of OSCAR is the second one, and is versioned using CalVer.\nTable of Contents Features Changes Table OSCAR Schema v1.1.0 Changes File formats Features These are the versions of tooling, schemes and data\nCommonCrawl version: February/March 2021 (2021.10) OSCAR Schema version: v1.1 : Incorporates metadata in a backward compatible manner. Ungoliant version: v1 : New generation tool, faster and better documented/tested than the previous one: goclassy. Changes As per OSCAR Schema v1.1, each document/record has associated metadata. New languages: Manx, Rusyn, Scots and West Flemish. Their size and quality still has to be assessed. Removed languages: Central Bikol and Cantonese. Cantonsese was of a very low quality. Central Bikol corpus is still available on OSCAR 2019. Table Language OSCAR 2019 OSCAR 2019 deduplicated OSCAR 21.09 OSCAR 21.09 deduplicated Issues af Afrikaans 251MB 170MB 258MB 157MB sq Albanian 2GB 1GB 3GB 1GB am Amharic 377MB 215MB 405MB 241MB ar Arabic 87GB 33GB 69GB 35GB an Aragonese 1MB 822KB 1MB 608KB hy Armenian 3GB 1GB 4GB 1GB as Assamese 117MB 73MB 135MB 95MB ast Asturian 2MB 2MB 7MB 4MB av Avaric 418KB 331KB 421KB 325KB az Azerbaijani 2GB 1GB 3GB 1GB bn Bangla 10GB 6GB 14GB 7GB ba Bashkir 133MB 93MB 110MB 77MB eu Basque 889MB 358MB 900MB 503MB bar Bavarian 507B 507B 2KB 1KB be Belarusian 1GB 1GB 2GB 1GB bh Bihari languages 112KB 34KB 579KB 120KB bpy Bishnupriya 4MB 1MB 11MB 4MB bs Bosnian 459KB 120KB 310KB 175KB br Breton 29MB 16MB 49MB 23MB bg Bulgarian 33GB 14GB 34GB 15GB my Burmese 2GB 1GB 2GB 1GB yue Cantonese 3KB 2KB - - ca Catalan 8GB 4GB 13GB 6GB ceb Cebuano 40MB 24MB 81MB 58MB bcl Central Bikol 886B 886B - - ckb Central Kurdish 509MB 236MB 784MB 367MB cbk Chavacano 521B 521B 168B 168B ce Chechen 8MB 6MB 29MB 20MB zh Chinese 544GB 267GB 500GB 266GB cv Chuvash 40MB 27MB 60MB 41MB kw Cornish 44KB 14KB 119KB 72KB hr Croatian 237MB 115MB 361MB 169MB cs Czech 56GB 25GB 72GB 33GB da Danish 16GB 10GB 18GB 10GB diq Dimli (individual language) 147B 147B 294B 147B dv Divehi 131MB 81MB 143MB 111MB nl Dutch 82GB 41GB 97GB 47GB mhr Eastern Mari 7MB 6MB 15MB 10MB arz Egyptian Arabic 68MB 34MB 48MB 21MB en English 2520GB 1294GB 2936GB 1342GB myv Erzya 1KB 1KB 29KB 2KB eo Esperanto 312MB 238MB 560MB 390MB et Estonian 5GB 2GB 7GB 3GB tl Filipino 601MB 426MB 699MB 383MB fi Finnish 28GB 13GB 35GB 20GB fr French 302GB 147GB 340GB 161GB gl Galician 650MB 402MB 989MB 549MB ka Georgian 3GB 1GB 6GB 2GB de German 330GB 155GB 433GB 184GB gom Goan Konkani 2MB 1MB 3MB 2MB el Greek 66GB 28GB 72GB 30GB gn Guarani 36KB 23KB 32KB 25KB gu Gujarati 1GB 756MB 1GB 950MB ht Haitian Creole 3KB 3KB 2KB 1KB he Hebrew 21GB 10GB 29GB 11GB hi Hindi 17GB 9GB 26GB 13GB hu Hungarian 42GB 18GB 60GB 29GB is Icelandic 1GB 887MB 2GB 1GB io Ido 151KB 133KB 276KB 221KB ilo Iloko 896KB 653KB 1MB 857KB id Indonesian 32GB 16GB 40GB 22GB ia Interlingua 678KB 368KB 291KB 172KB ie Interlingue 24KB 1KB 7KB 2KB ga Irish 91MB 62MB 131MB 69MB it Italian 146GB 73GB 192GB 94GB ja Japanese 231GB 112GB 208GB 96GB jv Javanese 675KB 598KB 858KB 728KB xal Kalmyk 115KB 114KB 62KB 62KB kn Kannada 1GB 1GB 2GB 1GB krc Karachay-Balkar 2MB 2MB 2MB 2MB kk Kazakh 2GB 1GB 3GB 1GB km Khmer 1GB 608MB 1GB 860MB kv Komi 2MB 1MB 1MB 588KB ko Korean 25GB 11GB 35GB 15GB ku Kurdish 98MB 62MB 152MB 108MB ky Kyrgyz 629MB 406MB 485MB 334MB lo Lao 181MB 118MB 287MB 163MB la Latin 26MB 8MB 103MB 9MB lv Latvian 4GB 1GB 6GB 2GB lez Lezghian 3MB 3MB 2MB 2MB li Limburgish 29KB 27KB 76KB 54KB lt Lithuanian 9GB 4GB 12GB 5GB jbo Lojban 753KB 694KB 929KB 731KB lmo Lombard 454KB 444KB 1MB 1MB nds Low German 18MB 13MB 25MB 17MB dsb Lower Sorbian 13KB 7KB 31KB 14KB lb Luxembourgish 30MB 21MB 54MB 37MB mk Macedonian 2GB 1GB 3GB 1GB mai Maithili 324KB 10KB 685KB 24KB mg Malagasy 21MB 13MB 59MB 38MB ms Malay 116MB 43MB 146MB 60MB ml Malayalam 5GB 2GB 4GB 2GB mt Maltese 24MB 17MB 51MB 26MB gv Manx - - 1KB 907B mr Marathi 2GB 1GB 3GB 1GB mzn Mazanderani 708KB 617KB 1MB 1MB min Minangkabau 622KB 317KB 8MB 1MB xmf Mingrelian 6MB 4MB 16MB 10MB mwl Mirandese 1KB 1KB 3KB 2KB mn Mongolian 2GB 879MB 1GB 912MB nah Nahuatl languages 11KB 10KB 34KB 21KB nap Neapolitan 17KB 13KB 1KB 1KB ne Nepali 1GB 1GB 3GB 2GB new Newari 5MB 4MB 6MB 4MB frr Northern Frisian 4KB 4KB 7KB 5KB lrc Northern Luri 77KB 64KB 183B 183B no ‚Ä¶","date":1630587731,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644600590,"objectID":"823a341b5f42a1cd0e53d590cddc5029","permalink":"https://oscar-project.org/post/oscar-v21-09/","publishdate":"2021-09-02T15:02:11+02:00","relpermalink":"/post/oscar-v21-09/","section":"post","summary":"The September, 2021 version of the OSCAR Corpus.","tags":[],"title":"OSCAR 21.09","type":"post"},{"authors":["Pedro Ortiz Suarez"],"categories":["corpus","update"],"content":"OSCAR 2019 is the original 2019 release of the OSCAR corpus. It has been generated from Common Crawl corpus using the goclassy architecture.\nTable of Contents Features Citing OSCAR The Unshuffled OSCAR Downloading OSCAR License Notice and take down policy Models Featured Models Features OSCAR 2019 is shuffled at line level and no metadata is provided. Thus it is mainly intended to be used in the training of unsupervised language models for NLP.\nData is distributed by language in both original and deduplicated form.\nIf you need the unshuffled version of OSCAR, please contact us using the contact form. Please include your name, affiliation, contact details, which languages do you need and a brief description of how you intend to use OSCAR. You can also download it using HuggingFace‚Äôs datasets library.\nEven though OSCAR is not Postcardware, we do appreciate when our users send us a postcard. If you want to send us one, you can find the address in the contact section down below.\nCiting OSCAR If you use OSCAR to train a language model, text generation model or any other ML model in general please consider citing our latest paper:\n@inproceedings{ortiz-suarez-etal-2020-monolingual, title = \u0026#34;A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages\u0026#34;, author = \u0026#34;Ortiz Su{\\\u0026#39;a}rez, Pedro Javier and Romary, Laurent and Sagot, Beno{\\^\\i}t\u0026#34;, booktitle = \u0026#34;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\u0026#34;, month = jul, year = \u0026#34;2020\u0026#34;, address = \u0026#34;Online\u0026#34;, publisher = \u0026#34;Association for Computational Linguistics\u0026#34;, url = \u0026#34;https://www.aclweb.org/anthology/2020.acl-main.156\u0026#34;, pages = \u0026#34;1703--1714\u0026#34;, abstract = \u0026#34;We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.\u0026#34;, } The Unshuffled OSCAR If you need a copy of any of the unshuffled sub-corpora, please contact us using the contact form down below. Please include your name, affiliation, contact details, which languages do you need and a brief description of how you intend to use OSCAR. We will evaluate your request and answer accordingly.\nThe unshuffled OSCAR is now available in HuggingFace‚Äôs datasets library They have obtained our permission to redistribute the unshuffled OSCAR and they allow users to download a corpus all at once as opposed to file by file. You can get more information about how to download OSCAR using their library by visiting OSCAR‚Äôs dataset card.\nDownloading OSCAR All the data is distributed by language, both the original and the deduplicated versions of the data are available. To download a file just click the desired link on the table below. Languages are split in shards of around 700MB, these shards are standalone. A plain text file with checksums is also provided.\nThe OSCAR corpus is yet to be filtered, so please be careful when using it, specially for text generation tasks! To see which sub-corpora have been audited, please refer to the list of publications above for more information.\nYou‚Äôll be asked to create an HumanID account in order to download a corpus. This is intended, and we do it in order to limit traffic and reduce abuse of the infrastructure. The OSCAR corpus is hosted by Huma-Num, you can read more about them on their website.\nAll sizes are for the uncompressed files.\nLanguage Words original Size original File original Words deduplicated Size deduplicated File deduplicated Afrikaans 43,482,801 241M af 29,533,437 163M af Albanian 374,196,110 2.3G sq 186,856,699 1.2G sq Alemannic 841,750 5.0M als 459,001 2.8M als Amharic 28,301,601 360M am 16,086,628 206M am Arabic 8,117,162,828 82G ar 3,171,221,354 32G ar Aragonese 52,896 1.3M an 45,669 801K an Armenian 273,919,388 3.7G hy 110,196,043 1.5G hy Assamese 6,956,663 113M as 4,366,570 71M as Asturian 381,005 2.4M ast 325,237 2.0M ast Avaric 24,720 409K av 19,478 324K av Azerbaijani 322,641,710 2.8G az 167,742,296 1.5G az Bashkir 9,796,764 128M ba 6,922,589 90M ba Basque 120,456,652 848M eu 45,359,710 342M eu Bavarian 399 503 bar 399 503 bar Belarusian 144,579,630 1.8G be 83,499,037 1.1G be Bengali 623,575,733 11G bn 363,766,143 5.8G bn Bihari 8,848 110K bh ‚Ä¶","date":1630584460,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645032036,"objectID":"1562a830fa4b3cacce5c9f97d421126c","permalink":"https://oscar-project.org/post/oscar-2019/","publishdate":"2021-09-02T14:07:40+02:00","relpermalink":"/post/oscar-2019/","section":"post","summary":"The first OSCAR Corpus version from 2019.","tags":[],"title":"OSCAR 2019","type":"post"},{"authors":["Julien Abadji","Pedro Ortiz Suarez","Laurent Romary","Beno√Æt Sagot"],"categories":null,"content":"","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630676828,"objectID":"6d8be031bf26602ae3c68247d44cc66b","permalink":"https://oscar-project.org/publication/2021/cmlc9/ungoliant/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021/cmlc9/ungoliant/","section":"publication","summary":"We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data.","tags":null,"title":"Ungoliant: An Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus","type":"publication"},{"authors":["Julien Abadji","Pedro Ortiz Suarez","Laurent Romary","Beno√Æt Sagot"],"categories":null,"content":"","date":1626084e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630678232,"objectID":"58c1d9232e4b49bfb132a82fa88048dc","permalink":"https://oscar-project.org/talk/cmlc9/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/cmlc9/","section":"talk","summary":"We propose a new pipeline that is faster, modular, parameterizable, and well documented. We use it to create a corpus similar to OSCAR but larger and based on recent data.","tags":[],"title":"Ungoliant: An Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus","type":"talk"},{"authors":["Isaac Caswell","Julia Kreutzer","Lisa Wang","Ahsan Wahab","Daan van Esch","Nasanbayar Ulzii-Orshikh","Allahsera Tapo","Nishant Subramani","Artem Sokolov","Claytone Sikasote","Monang Setyawan","Supheakmungkol Sarin","Sokhar Samb","Beno√Æt Sagot","Clara Rivera","Annette Rios","Isabel Papadimitriou","Salomey Osei","Pedro Ortiz Suarez","Iroro Orife","Kelechi Ogueji","Rubungo Andre Niyongabo","Toan Q. Nguyen","Mathias M√ºller","Andr√© M√ºller","Shamsuddeen Hassan Muhammad","Nanda Muhammad","Ayanda Mnyakeni","Jamshidbek Mirzakhalov","Tapiwanashe Matangira","Colin Leong","Nze Lawson","Sneha Kudugunta","Yacine Jernite","Mathias Jenny","Orhan Firat","Bonaventure F. P. Dossou","Sakhile Dlamini","Nisansa de Silva","Sakine √áabuk Ballƒ±","Stella Biderman","Alessia Battisti","Ahmed Baruwa","Ankur Bapna","Pallavi Baljekar","Israel Abebe Azime","Ayodele Awokoya","Duygu Ataman","Orevaoghene Ahia","Oghenefego Ahia","Sweta Agrawal","Mofetoluwa Adeyemi"],"categories":null,"content":"","date":1616371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630676828,"objectID":"17643513f55a6e0896fc47473a68a064","permalink":"https://oscar-project.org/publication/2021/africanlp/quality/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021/africanlp/quality/","section":"publication","summary":"We audit 5 multilingual corpora, finding that lower-resource corpora have systematic issues.","tags":null,"title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets","type":"publication"},{"authors":["Pedro Ortiz Suarez","Laurent Romary","Beno√Æt Sagot"],"categories":null,"content":"","date":1594033200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630678232,"objectID":"b5ca6c7c105c6b1158aea6bcd07eaa03","permalink":"https://oscar-project.org/talk/acl2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/acl2020/","section":"talk","summary":"We explore the impact of the training corpus on contextualized word embeddings in five mid-resource languages.","tags":[],"title":"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages","type":"talk"},{"authors":["Pedro Ortiz Suarez","Laurent Romary","Beno√Æt Sagot"],"categories":null,"content":"","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630676828,"objectID":"1a9e4c9337d222e180c1532ee82db482","permalink":"https://oscar-project.org/publication/2020/acl/elmos/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2020/acl/elmos/","section":"publication","summary":"We explore the impact of the training corpus on contextualized word embeddings in five mid-resource languages.","tags":null,"title":"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages","type":"publication"},{"authors":["Pedro Ortiz Suarez","Yoann Dupont","Benjamin Muller","Laurent Romary","Beno√Æt Sagot"],"categories":null,"content":"","date":1589587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630676828,"objectID":"123816d2a0b8bc603764d9e67953db60","permalink":"https://oscar-project.org/publication/2020/lrec/ner/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2020/lrec/ner/","section":"publication","summary":"We explore convert the NER annotations of the French TreeBank to a more user-friendly format and establish a new state of the art for French NER.","tags":null,"title":"Establishing a New State-of-the-Art for French Named Entity Recognition","type":"publication"},{"authors":["Pedro Ortiz Suarez","Beno√Æt Sagot","Laurent Romary"],"categories":null,"content":"","date":1563787800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630678232,"objectID":"6a3aeb492867571bc30acaa749b486aa","permalink":"https://oscar-project.org/talk/cmlc7/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/cmlc7/","section":"talk","summary":"We propose a new pipeline to filter, clean and classify Common Crawl by language, we publish the final corpus under the name OSCAR.","tags":[],"title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures","type":"talk"},{"authors":["Pedro Ortiz Suarez","Beno√Æt Sagot","Laurent Romary"],"categories":null,"content":"","date":1563753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630676828,"objectID":"25f6289c4f5df45b3b287ebb8df27a65","permalink":"https://oscar-project.org/publication/2019/clmc7/asynchronous/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019/clmc7/asynchronous/","section":"publication","summary":"We propose a new pipeline to filter, clean and classify Common Crawl by language, we publish the final corpus under the name OSCAR.","tags":null,"title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures","type":"publication"}]